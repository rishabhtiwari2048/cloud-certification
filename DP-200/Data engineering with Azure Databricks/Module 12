# Create production workload with Azure Data Factory

Schedule Databricks jobs in a data factory pipeline
  Azure Data Factory is a cloud-based ETL and data integration service that allows you to create data-driven workflows for orchestrating data movement and transforming data at scale. Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores.
  A data factory can have one or more pipelines. A pipeline is a logical grouping of activities that together perform a task. For example, a pipeline could contain a set of activities that ingest and clean log data, and then kick off a mapping data flow to analyze the log data. The pipeline allows you to manage the activities as a set instead of each one individually. You deploy and schedule the pipeline instead of the activities independently.
  While here we will show scheduling a notebook with the Data Factory UI, you can also schedule .jar and .py files, taking advantage of the much lower cost of Data Engineering vs. interactive clusters.

Please go through the module to check how azure databricks is linked to Azure Data Factory.
