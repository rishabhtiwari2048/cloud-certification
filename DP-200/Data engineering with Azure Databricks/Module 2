# Spark Architecture Fundamentals

Understand the architecture of Azure Databricks spark cluster
  To gain a better understanding of how to develop with Azure Databricks, it is important to understand the underlying architecture. We will look at two aspects of the Databricks architecture: the Azure Databricks service and Apache Spark clusters

  a) High Level Overview
      From a high level, the Azure Databricks service launches and manages Apache Spark clusters within your Azure subscription. Apache Spark clusters are groups of computers that are treated as a single computer and handle the execution of commands issued from notebooks. Using a master-worker type architecture, clusters allow processing of data to be parallelized across many computers to improve scale and performance. They consist of a Spark Driver (master) and worker nodes. The driver node sends work to the worker nodes and instructs them to pull data from a specified data source.

      /*IMPORTANT*/: Driver programs access Apache Spark through a SparkSession object regardless of deployment location.

      Microsoft Azure manages the cluster, and auto-scales it as needed based on your usage and the setting used when configuring the cluster. Auto-termination can also be enabled, which allows Azure to terminate the cluster after a specified number of minutes of inactivity.

   b) Under the covers
      Now let's take a deeper look under the covers. When you create an Azure Databricks service, a "Databricks appliance" is deployed as an Azure resource in your subscription. At the time of cluster creation, you specify the types and sizes of the virtual machines (VMs) to use for both the Driver and Worker nodes, but Azure Databricks manages all other aspects of the cluster.

      You also have the option of using a Serverless Pool. A Serverless Pool is self-managed pool of cloud resources that is auto-configured for interactive Spark workloads. You provide the minimum and maximum number of workers and the worker type, and Azure Databricks provisions the compute and local storage based on your usage.


Understand the architecture of Spark job
    /*IMPORTANT*/ : Before we break down these components in detail, let us summarize the fundamentals of a Spark architecture. As you may know, Spark is a Distributed computing environment. The unit of distribution is a Spark Cluster. Every Cluster has a Driver and one or more executors. Work submitted to the Cluster is split into as many independent Jobs as needed. This is how work is distributed across the Cluster's nodes. Jobs are further subdivided into tasks. The input to a job is partitioned into one or more partitions. These partitions are the unit of work for each slot. In between tasks, partitions may need to be re-organized and shared over the network.

    The cluster: Drivers, executors, slots & tasks

       It is the JVM in which our application runs.
       The secret to Spark's awesome performance is parallelism.
       The first level of parallelization is the Executor - a Java virtual machine running on a node, typically, one instance per node.
       The second level of parallelization is the Slot - the number of which is determined by the number of cores and CPUs of each node.
       Each Executor has a number of Slots to which parallelized Tasks can be assigned to it by the Driver.
       The JVM is naturally multithreaded, but a single JVM, such as our Driver, has a finite upper limit.
       By creating Tasks, the Driver can assign units of work to Slots for parallel execution.
       Additionally, the Driver must also decide how to partition the data so that it can be distributed for parallel processing (not shown here).
       Consequently, the Driver is assigning a Partition of data to each task - in this way each Task knows which piece of data it is to process.
       Once started, each Task will fetch from the original data source the Partition of data assigned to it.

    Jobs and Stages
        Each parallelized action is referred to as a Job.
        The results of each Job (parallelized/distributed action) is returned to the Driver.
        Depending on the work required, multiple Jobs will be required.
        Each Job is broken down into Stages.

    Cluster Management
      At a much lower level, Spark Core employs a Cluster Manager that is responsible for provisioning nodes in our cluster.
      Databricks provides a robust, high-performing Cluster Manager as part of its overall offerings.
      In each of these scenarios, the Driver is [presumably] running on one node, with each Executors running on N different nodes.
      For the sake of this learning path, we don't need to concern ourselves with cluster management, thanks to Azure Databricks.
