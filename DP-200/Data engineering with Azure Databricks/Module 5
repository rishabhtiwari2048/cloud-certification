# Describe lazy evaluation and other performance features in Azure Databricks

Describe the difference between eager and lazy execution

  Laziness by design
    Fundamentals to Apache spark are notion that,
      Transformations are LAZY
      Actions are EAGER

Describe the fundamentals of how catalyst optimizer works
  Among the most powerful components of Spark are Spark SQL. At its core lies the Catalyst optimizer. This extensible query optimizer supports both rule-based and cost-based optimization.
  /*Important*/
  When you execute code, Spark SQL uses Catalyst's general tree transformation framework in four phases, as shown below: (1) analyzing a logical plan to resolve references, (2) logical plan optimization, (3) physical planning, and (4) code generation to compile parts of the query to Java bytecode. In the physical planning phase, Catalyst may generate multiple plans and compare them based on cost. All other phases are purely rule-based.
  Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:
      Easily add new optimization techniques and features to Spark SQL
      Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.)

Define and Identify Actions
  Actions In production code, actions will generally write data to persistent storage using the DataFrameWriter discussed in other Azure Databricks learning path modules.

Define and Identify Transformations
Transformations have the following key characteristics:
   They eventually return another DataFrame.
   They are immutable - that is each instance of a DataFrame cannot be altered once it's instantiated.
   This means other optimizations are possible - such as the use of shuffle files (to be discussed in detail later)
   Are classified as either a Wide or Narrow operation

Types of Transformations
   A transformation may be wide or narrow.
    A WIDE transformation requires sharing data across workers
    A NARROW transformation can be applied per partition/worker with no need to share or shuffle data to other workers.

Describe performance enhancements enabled by shuffle operations and Tungsten
  In the previous unit, you explored actions and transformations. As opposed to narrow transformations, wide transformations cause data to shuffle between executors. This is because a wide transformation requires sharing data across workers. Pipelining helps us optimize our operations based on the differences between the two types of transformations.

  Pipelining
      a) Pipelining is the idea of executing as many operations as possible on a single partition of data.
      b) Once a single partition of data is read into RAM, Spark will combine as many narrow operations as it can into a single Task
      c) Wide operations force a shuffle, conclude a stage, and end a pipeline

  Shuffles
      a) Convert the data to the UnsafeRow, commonly referred to as Tungsten Binary Format.
      b) Write that data to disk on the local node - at this point the slot is free for the next task.
      c) Send that data across the wire to another executor
          Technically the Driver decides which executor gets which piece of data.
          Then the executor pulls the data it needs from the other executor's shuffle files.
      d) Copy the data back into RAM on the new executor
          The concept, if not the action, is just like the initial read "every" DataFrame starts with.
          The main difference being it's the 2nd+ stage.

UnsafeRow (Also known as Tungsten Binary Format)
      Sharing data from one worker to another can be a costly operation. Spark has optimized this operation by using a format called Tungsten. Tungsten prevents the need for expensive serialization and de-serialization of objects in order to get data from one JVM to another.The data that is "shuffled" is in a format known as UnsafeRow, or more commonly, the Tungsten Binary Format. UnsafeRow is the in-memory storage format for Spark SQL, DataFrames & Datasets.

      Advantages

        Compactness:
            Column values are encoded using custom encoders, not as JVM objects (as with RDDs).
            The benefit of using Spark 2.x's custom encoders is that you get almost the same compactness as Java serialization, but significantly faster encoding/decoding speeds. Also, for custom data types, it is possible to write custom encoders from scratch.
        Efficiency:
            Spark can operate directly out of Tungsten, without first deserializing Tungsten data into JVM objects.
