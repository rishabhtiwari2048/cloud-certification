# Design a data warehouse with Azure Synapse Analytics

  A data warehouse is a centralized relational database that integrates data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data. In the past this has involved complex processes, using different technologies including Big Data technologies and data integration tools to ingest and prepare the data before presenting it to applications. Azure Synapse Analytics is designed to simplify this process with limitless scale through one experience.

Understand Azure Synapse Analytics
  Azure Synapse Analytics provides a unified environment by combining the enterprise data warehouse of SQL, the Big Data analytics capabilities of Spark, and data integration technologies to ease the movement of data between both, and from external data sources. Using Azure Synapse Analytics, you are able to ingest, prepare, manage, and serve data for immediate BI and machine learning needs more easily.

  Using SQL Analytics and SQL Pools in Azure Synapse Analytics, you will be able to work with modern data warehouse use cases to create database and tables. In addition, you can load data using PolyBase and query a data warehouse using the Massively Parallel Processing architecture. In fact, you can use any existing code that you have created for Azure SQL Data Warehouse with Azure Synapse Analytics seamlessly.

  Azure Synapse Analytics stores the incoming data into relational tables with columnar storage. This format significantly reduces the data storage costs and improves the query performance. Once information is stored in Azure Synapse Analytics, you can run analytics at massive scale. Compared to the traditional database systems, queries on Azure Synapse Analytics finish in a fraction of the time. Within this platform, SQL Analytics offers T-SQL for batch, streaming, and interactive processing of data

Data Warehousing with Azure Synapse Analytics

  SQL Pools using Data Warehouse Units
    With Azure Synapse Analytics. CPU, memory, and IO are bundled into units of compute scale called SQL pool. It is a normalized measure of compute resources and performance, and the size of SQL pool is determined by Data Warehousing Units (DWU). By changing your service level, you alter the number of DWUs that are allocated to the system. This in turn adjusts the performance, and the cost of your system. To pay for higher performance, you can increase the number of data warehouse units. To pay for less performance, reduce data warehouse units. Storage and compute costs are billed separately, so changing data warehouse units does not affect storage costs.

  Benefits of Data Warehousing using Azure Synapse Analytics
    It allows data to be ingested from a variety of data sources, and leverages a scale-out architecture to distribute computational processing of data across a large cluster of nodes, which can:

        Independently size compute power irrespective of the storage needs.
        Grow or shrink compute power without moving data.
        Pause compute capacity while leaving data intact, so you only pay for storage.
        Resume compute capacity during operational hours.

    The Azure Synapse Analytics capabilities are made possible due to the decoupling of computation and storage using the Massively Parallel Processing architecture.

Azure Synapse Analytics Features

    a) Workload Management : capability to prioritize the query workloads that take place on the server

        Workload management is managed by three related areas:

          1) Workload Groups: Workload Groups enable you to define the resources to isolate and reserve resources for its use.

          2) Workload Classification : Using T-SQL, you can create a workload classifier to map queries to a specific classifier. A classifier can define the level of importance of the request, so that it can be mapped to a specific workload group that has an allocation of specific resources during the execution of the query.

          3) Workload Importance : Workload importance is defined in the CREATE WORKLOAD CLASSIFIER command and allows higher priority queries to receive resources ahead of lower priority queries that are in the queue. By default, queries are released from the queue on a first-in, first-out basis as resources become available, but workload importance overrides this.

    b) Result-set Cache
        In scenarios where the same results are requested on a regular basis, result-set caching can be used to improve the performance of the queries that retrieve these results. When result-set caching is enabled, the results of the query are cached in the SQL pool storage. Result-set cache enables interactive response times for repetitive queries against tables with infrequent data changes. The result-set cache persists even if SQL pool is paused and resumed later, although the query cache is invalidated and refreshed when the underlying table data or query code changes. To ensure that the cache is fresh, the result cache is evicted on a regular basis on a time-aware least recently used algorithm (TLRU). You can set result-set caching on at the database level or at a session level

    c) Materialized Views
        A materialized view will pre-compute, store, and maintain data like a table. They are automatically updated when data in underlying tables are changed. Updating materialized views is a synchronous operation that occurs as soon as the data is changed. This auto caching functionality allows Azure Synapse Analytics Query Optimizer to consider using an indexed view even if the view is not referenced in the query.

    d) Continuous Improvement/Continuous Development (CI/CD) support through SQL Server Data Tools (SSDT).
        Database project support in SQL Server Data Tools (SSDT) allows teams of developers to collaborate over a version-controlled Azure Synapse Analytics, and track, deploy, and test schema changes.

Types of Solution Scenarios
    There are two common types of solution workloads that can be supported by Azure Synapse Analytics.

    1) Modern Data Warehouse workloads: A Modern Data Warehouse is a centralized data store that provides analytics and decision support services across the whole enterprise using structured, unstructured, or streaming data sources. Data flows into the warehouse from multiple transactional systems, relational databases, and other data sources on a periodic basis. The stored data is used for historical and trend analysis reporting. The data warehouse acts as a central repository for many subject areas and contains the "single source of truth."

    2) Advanced Analytical Workloads : You can perform advanced analytics in the form of predictive or preemptive analytics using Azure Synapse Analytics, with the tight integration with Spark, and the hybrid data integration engine with Data Integration. In a cloud data solution, data is ingested into big data stores from a variety of sources. Once in a big data store, Hadoop, Spark, and machine learning algorithms prepare and train the data. When the data is ready for complex analysis, SQL Analytics uses PolyBase to query the big data stores.

    PolyBase uses standard T-SQL queries to bring the data into SQL Analytics tables. As a result, the worlds of the Modern Data Warehouse and the Advanced Analytical Workloads are brought together

Understand Massively Parallel Processing Concepts
  Azure Synapse Analytics uses a node-based architecture. Applications connect and issue T-SQL commands to a Control node, which is the single point of entry for the data warehouse. The Control node runs the Massively Parallel Processing (MPP) engine which optimizes queries for parallel processing and then passes operations to Compute nodes to do their work in parallel. The Compute nodes store all user data in Azure Storage and run the parallel queries. The Data Movement Service (DMS) is a system-level internal service that moves data across the nodes as necessary to run queries in parallel and return accurate results.

  There are 3 basic components in the Azure Synapse Analytics Architecture

  a) Control Node : The Control node is the brain of the data warehouse. It is the front end that interacts with all applications and connections. The MPP engine runs on the Control node to optimize and coordinate parallel queries. When one submits a T-SQL query to the Azure Synapse Analytics, the Control node transforms it into queries that run against each distribution in parallel.

  b) Compute Node : The Compute nodes provide the computational power. Distributions map to compute nodes for processing. As one pays for more compute resources, SQL Data Warehouse re-maps the distributions to the available compute nodes. The number of compute nodes ranges from 1 to 60 and is determined by the service level for the data warehouse.

  c) Data Movement Service : Data Movement Service (DMS) is the data transport technology that coordinates data movement between the compute nodes. When SQL Data Warehouse runs a query, the work is divided into 60 smaller queries that run in parallel. Each of the 60 smaller queries runs on one of the underlying data distributions. A distribution is the basic unit of storage and processing for parallel queries that run on distributed data. Some queries require data movement across nodes to ensure the parallel queries return accurate results. When data movement is required, DMS ensures the right data gets to the correct location.

Table Geometries
  Azure Synapse Analytics uses Azure Storage to store data. Since the data is stored and managed by Azure Storage, Azure Synapse Analytics charges separately for the storage consumption. The data itself is sharded into distributions to optimize the performance of the system. One can choose which sharding pattern to use to distribute the data when one defines the table. Azure Synapse Analytics supports these sharding patterns:

  Hash
  Round Robin
  Replicate

  Here are some tips that can help you choose a strategy:

  1)  Start with Round Robin, but aspire to a Hash distribution strategy to take advantage of a massively parallel architecture.
  2)  Make sure that common hash keys have the same data format.
  3)  Do not distribute on varchar format.
  4)  Dimension tables with a common hash key to a fact table with many join operations can be hash distributed.
  5)  Use sys.dm_pdw_request_steps to analyze data movements behind queries, monitor the time broadcast and shuffle operations take. This is helpful to review your distribution strategy.

  a) Hash Distributed Tables : A hash distributed table can deliver the highest query performance for joins and aggregations on large tables.
  To shard data into a hash-distributed table, Azure Synapse Analytics uses a hash function to assign each row to one distribution deterministically. In the table definition, one of the columns is designated as the distribution column. The hash function uses the values in the distribution column to assign each row to a distribution.

  b) Round Robin Distributed Tables : A round-robin table is the most straightforward table to create and delivers fast performance when used as a staging table for loads.

  A round-robin distributed table distributes data evenly across the nodes but without any further optimization. A distribution is first chosen at random, and then buffers of rows are assigned to distributions sequentially. It is quick to load data into a round-robin table, but query performance can often be better with hash distributed tables. Joins on round-robin tables require reshuffling data, and this takes additional time.

  c) Replicated Tables : A replicated table provides the fastest query performance for small tables.

  A table that is replicated caches a full copy on each compute node. Consequently, replicating a table removes the need to transfer data among compute nodes before a join or aggregation. Replicated tables are best utilized with small tables. Extra storage is required, and there are additional overheads that are incurred when writing data which make large tables impractical.


Exercise - Create a data warehouse in Azure Synapse Analytics by using the portal
======================================================================================================
                                      EXERCISE
======================================================================================================
Please refer unit 7 of Module 1
